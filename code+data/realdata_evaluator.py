# this is similar to enviroment.py
import pandas as pd
import numpy as np
import data_preprocessing

import sys
sys.path.insert(0, 'plot')
import average_reward_yahoo

def get_offline_data(offline_df, N_offline=None):
	''' to be compatible with old APIs
	'''
	offline_dict = dict()
	for colname in list(offline_df):
		if N_offline:
			offline_dict[colname] = offline_df[colname].tolist()[0:N_offline]
		else:
			offline_dict[colname] = offline_df[colname].tolist()
	return offline_dict

class OnlineEvaluator:
	# evaluate the performance of the algorithm by logged data, not by interactive environment
	def __init__(self, experimental_df, context_names, treatment_name, outcome_name,\
			choice_names=None):
		''' the experimental data is generated by a random process
			the experimental data is in a format of dataframe
		'''
		self.experimental_df = experimental_df
		self.context_names = context_names
		self.treatment_name = treatment_name
		self.outcome_name = outcome_name

		self.choice_names = choice_names # only choose from the specified columns


	def evaluate_algorithm(self, algorithm_runner, T): # added on 1-29
		''' we only know the reward, the regret should be calculated by A/B sub-group summary
		'''
		reward_vec = []
		t = 0
		has_done_match = False
		unmatched_count = 0

		# added on 2-3, in longer runs, reuse the samples to be evaluated
		while t < T:
			self.experimental_df = self.experimental_df.sample(frac=1).reset_index(drop=True)
			for index, row in self.experimental_df.iterrows():
				if t >= T: # only consider T rounds
					break

				context_vec = []
				for context_name in self.context_names:
					context_vec.append( row[context_name] )

				choice_vec = []
				for choice_name in self.choice_names:
					if row[choice_name] != None: # may be an empty column
						choice_vec.append( row[choice_name] )

				if has_done_match == False: # for contextual like linUCB, we only want to match for once
					action = algorithm_runner.real_draw_arm(context_vec, choice_vec, do_match=True, update_pending=False) 
					if algorithm_runner.algorithm.contextual == True: # for contextual like linUCB, we only want to match for once
						has_done_match = True
				else:
					action = algorithm_runner.real_draw_arm(context_vec, choice_vec, do_match=False, update_pending=False)
				# print ('proposed action:', action)
				# print ('real action:', row[self.treatment_name])
				# print ('real context:', context_vec)
				if algorithm_runner.algorithm.__class__.__name__ == 'DisjointLinUCB':
					action_in_data = algorithm_runner.algorithm.article2idx[row[self.treatment_name]] # transform the article number(large) to index(small)
				else:
					action_in_data = row[self.treatment_name]
				if action == action_in_data:
					reward = row[self.outcome_name]

					################ added on 2-2 ##################
					# the linear reward is the article_features * the user features
					#linear_reward = np.dot(context_vec, algorithm_runner.algorithm.articles_feature[action])
					# print (context_vec, algorithm_runner.algorithm.articles_feature[action])
					# print ('reward:', reward, 'linear_reward:', linear_reward)

					#reward = linear_reward

					#print (action_in_data)
					################################################

					algorithm_runner.real_feedback(reward)
					# update the pending in the match machine
					algorithm_runner.match_machine.get_pending_action(context_vec, update_pending=True)

					#reward = average_reward_yahoo.average_reward_vec[action] # for debug
					reward_vec.append(reward)
					# TODO: find the best action in all choices
					# max_reward = -np.inf
					# for action in choice_vec:
					# 	if np.isnan(action):
					# 		continue
					# 	tmp_reward = average_reward_yahoo.average_reward_vec[int(action)]
					# 	if tmp_reward > max_reward:
					# 		max_reward = tmp_reward
					# reward_vec.append(max_reward)
					
					t += 1

				else:
					unmatched_count += 1
					if unmatched_count % 1000 == 0:
						print ("unmatched_count:", unmatched_count, "percentage:", unmatched_count/(t+unmatched_count))
					#print ('actions do not match, skip this sample') # skip this row
			
		return reward_vec


